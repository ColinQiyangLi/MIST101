{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIST101 Pratical 5: Mixture of Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro: \n",
    "In this tutorial you'll learn how to train an unsupervised model with maximium likelihood estimation (MLE) / maximum a posteriori (MAP) on MNIST. The model we'll use is a mixture of Bernoulli. The word **mixture** comes from the fact that we have a latent variable $z$ that could take discrete integer values ranging from 1 to 10. Our hope is that eventually, our model could learn a conditional distribution $p(x|z, \\theta)$, s.t. given the specific value of $z$ it could be able to present us a sensible probability mass function which is a product of Bernoullis that match that of any digit (1 to 10) we are modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recap of model definition: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(z)$ is defined to be a uniform categorical random variable which could take values 1,2,...10\n",
    "\n",
    "- $p( x_d | z, \\theta_d)$ is defined to be a Bernoulli distribution ($x_d$ is the $d$-th pixel):\n",
    "\n",
    "$$p( x_d | z, \\theta_d) = {\\theta_d}^{x_d}  (1 - \\theta_d)^{(1 - x_d)} $$\n",
    "\n",
    "- for numerical stability, we could write everything is log-space:\n",
    "\n",
    "$$\\log p( x_d | z, \\theta_d) = x_d \\log \\theta_d + (1 - x_d)\\log (1 - \\theta_d) $$\n",
    "\n",
    "- if we perform MLE, the objective we want to maximize is ($x^{(i)}$ denotes the $i$-th data point):\n",
    "\n",
    "$$\\sum_{i} \\log  \\sum_{z^{(i)}} p(z^{(i)}) \\prod_{d} p(x_d^{(i)} | z^{(i)}, \\theta) \\tag{1} $$\n",
    "\n",
    "- for numerical stability, we could use the numpy/tensorflow builtin stable version of **logsumexp**:\n",
    "\n",
    "$$\\text{logsumexp}(v_1, v_2, ..., v_n) = \\log \\sum_{i} \\exp(v_i)$$\n",
    "\n",
    "- this means when we want to compute a logsum, we could first take the log of each component and then pass to logsumexp:\n",
    "\n",
    "$$\\text{logsum}(u_1, u_2, ..., u_n) = \\text{logsumexp}(\\log u_1, \\log u_2, ..., \\log u_n)$$\n",
    "\n",
    "- our loss function (1) could be rewritten as:\n",
    "\n",
    "$$\\sum_{i} \\text{logsumexp} (\\text{logit}_1, \\text{logit}_2, ..., \\text{logit}_n ) \\tag{2} $$\n",
    "\n",
    "$$\\text{logit}_j = \\log p(z^{(i)}=j) \\prod_{d} p(x_d^{(i)} | z^{(i)}=j, \\theta) = \\log \\frac{1}{10} + \\sum_d \\log p(x_d^{(i)} | z^{(i)} = j , \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "train = mnist.train.images\n",
    "test  = mnist.test.images\n",
    "\n",
    "# in place binarization without copy\n",
    "train[train > 0.5] = 1. ; train[train <= 0.5] = 0.\n",
    "test[test > 0.5] = 1. ; test[test <= 0.5] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feel free to tweak the hyperparameters\n",
    "lr = 1e-4                                      # learning rate for optimizer\n",
    "categories = 10                                # number of categories for latent z\n",
    "epochs = 10                                    # number of epochs for training\n",
    "batch_size = 128                               # batch size\n",
    "train_iters = train.shape[0] // batch_size     # number of iterations each epoch\n",
    "log_every = 1                                  # print training status every such epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "theta = tf.get_variable(name='theta', shape=[categories, 784])\n",
    "\n",
    "# TODO: define loss function here according to equation (2) above\n",
    "# Hint: you might find tf.reduce_logsumexp useful\n",
    "loss = tf.reduce_sum(theta)  # this is just a stub, you should modify this\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(-loss)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(train_iters):\n",
    "            batch = train[i*batch_size:(i+1)*batch_size]\n",
    "            sess.run(train_op, feed_dict={x: batch})\n",
    "        \n",
    "        if e % log_every == 0:\n",
    "            stats = sess.run(-loss, feed_dict={x: test})\n",
    "            print ('aggregate marginal log-likelihood on test set %.4f' % stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
